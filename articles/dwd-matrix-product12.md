---
title: "è¡Œåˆ—ç©çŠ¶æ…‹ã«ã¤ã„ã¦è€ƒãˆã‚‹ (12) â€” ãƒ†ãƒ³ã‚½ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ MNIST åˆ†é¡"
emoji: "ğŸ“ˆ"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["math", "PyTorch", "TensorNetwork"]
published: true
---

# ç›®çš„

æ–‡çŒ® [E] arXiv:1906.06329ã€Œ[TensorNetwork for Machine Learning](https://arxiv.org/abs/1906.06329)ã€ã‚’èª­ã‚“ã§ PyTorch ã§å®Ÿè£…ã—ã¦ã¿ãŸã®ã§è¨˜äº‹ã«ã™ã‚‹ã€‚

# ãƒ†ãƒ³ã‚½ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ç”»åƒåˆ†é¡å™¨

ä»¥ä¸‹ã®ã‚ˆã†ãªãƒ†ãƒ³ã‚½ãƒ«ãƒˆãƒ¬ã‚¤ãƒ³ã‚’ç”¨ã„ãŸç”»åƒåˆ†é¡ãƒ¢ãƒ‡ãƒ«ã‚’è€ƒãˆã‚‹ã€‚

![](/images/dwd-matrix-product12/001.png =400x)

è«–æ–‡ã® FIG. 2. ã®ãƒ†ãƒ³ã‚½ãƒ«ã®ãƒãƒ¼ãƒ‰ã«é©å½“ã«åå‰ã‚’ä»˜ã‘ã‚‹ã¨ã“ã®ã‚ˆã†ãªæ„Ÿã˜ã«ãªã‚‹ã€‚

$$
\begin{align*}
T_{i_1 i_2 i_3 i_4 i_5 i_6}^{\ell} = \sum_{\alpha_1,\alpha_2,\alpha_3,\alpha_4,\alpha_5,\alpha_6=0}^9 A^{(1)}_{i_1 \alpha_1} A^{(2)}_{i_2 \alpha_1 \alpha_2} A^{(3)}_{i_3 \alpha_2 \alpha_3} F^{\ell}_{\alpha_3 \alpha_4} A^{(4)}_{i_4 \alpha_4 \alpha_5} A^{(5)}_{i_5 \alpha_5 \alpha_6} A^{(6)}_{i_6 \alpha_6}
\tag{1}
\end{align*}
$$

ã“ã“ã§ã€$F^{\ell}_{\alpha_3 \alpha_4}$ ã¯è«–æ–‡ã§è¨€ã†ã€Œ_â€œlabelâ€ node_ã€ã§ã©ã“ã«å…¥ã‚Œã¦ã‚‚è‰¯ã„ãŒã€è«–æ–‡ã®å›³ã®ã‚ˆã†ã«çœŸã‚“ä¸­ã«å·®ã—è¾¼ã‚“ã ã€‚ä»Šå›ã¯çš†ãŒå¤§å¥½ããª MNIST

![](/images/dwd-matrix-product12/002.png =400x)

ã‚’æƒ³å®šã—ã¦ã„ã‚‹ã®ã§ $\ell \in \{0, \ldots, 9\}$ ã¨ãªã‚‹ã€‚$i_j \in \{0,1\}$ ã¯ç”»ç´ ç©ºé–“ã®ãƒ‡ãƒ¼ã‚¿ã¨ç¸®ç´„ã™ã‚‹ãŸã‚ã® â€œè„šâ€ ã§ã‚ã‚‹ã€‚ãƒ†ãƒ³ã‚½ãƒ«ãƒˆãƒ¬ã‚¤ãƒ³é–“ã®ä»®æƒ³ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®çµåˆæ¬¡å…ƒã‚’ $\chi = 10$ ã¨ã—ãŸã€‚2 æ¬¡å…ƒç”»ç´ ç©ºé–“ã¯ $X = [0, 1]^2$ ã§ã‚ã‚‹ãŒã€ç‰¹å¾´ãƒãƒƒãƒ— $\Phi$ ã«ã‚ˆã£ã¦ã€ç”»åƒã¯ $\mathscr{X} = X^{\otimes 6}$ ã«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã•ã‚Œã‚‹ã€‚

ä¾‹ãˆã° 6 ç”»ç´ ã®ç”»åƒã¯å¹³å¦åŒ–ã™ã‚‹ã“ã¨ã§ $\bm{p} = (p_1, p_2, p_3, p_4, p_5, p_6)$ ã¨ã„ã†ç”»ç´ ã®åˆ—ã«ãªã‚‹ã€‚ã“ã‚Œã‚’ç‰¹å¾´ãƒãƒƒãƒ— $\Phi$ ã«ã‚ˆã£ã¦ $\mathscr{X}$ ã«ã†ã¤ã™ã¨

$$
\begin{align*}
\Phi(\bm{p}) &= \Phi(p_1) \otimes \Phi(p_2) \otimes \Phi(p_3) \otimes \Phi(p_4) \otimes \Phi(p_5) \otimes \Phi(p_6) \\
&=: (x_1, x_2, x_3, x_4, x_5, x_6) \\
&= \bm{x}
\end{align*}
$$

ã¨ãªã‚‹ã€‚ã“ã‚Œã«å¯¾ã—ã¦ã€å…ˆã»ã©ã®ãƒ†ãƒ³ã‚½ãƒ«åˆ†é¡å™¨ $T^{\ell}$ ã‚’ä½œç”¨ã•ã›ã‚‹ã¨ $(f^{(\ell)}(\bm{x}))_{0 \le \ell \le 9} =(T^{\ell} \cdot \bm{x})_{0 \le \ell \le 9} \in \R^{10}$ ã¨ãªã‚‹ã€‚

## ã“ã“ã‹ã‚‰ã¯æ©Ÿæ¢°å­¦ç¿’å‹¢ãŠãªã˜ã¿

å¾Œã¯æ©Ÿæ¢°å­¦ç¿’ã§ã‚ˆãã‚ã‚‹ã‚ˆã†ã«ã€**softmax ã‚’é€šã—ã¦å„ãƒ©ãƒ™ãƒ«ã”ã¨ã®ç¢ºç‡ã‚’å‡ºã—ã¦ã€æ­£è§£ãƒ©ãƒ™ãƒ«ã¨ã®ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã‚’ã¨ã‚Œã°è‰¯ã„** ã¨ã„ã†ã“ã¨ã«ãªã‚‹ã€‚

ã“ã“ã¾ã§ã®å†…å®¹ã¯æ–‡çŒ® [S] ã§æ‰±ã‚ã‚Œã„ã‚‹ã‚‚ã®ã§ã‚ã‚‹ãŒã€è¨ˆç®—æ–¹æ³•ãŒ 1992 å¹´ã« S. White ã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸ DMRG (å¯†åº¦è¡Œåˆ—ç¹°ã‚Šè¾¼ã¿ç¾¤) ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã„ã†è¨ˆç®—ç‰©ç†ã®æ‰‹æ³•ã«ã‚ˆã£ã¦ã„ã‚‹ãŸã‚ã€æ–‡çŒ® [E]ï¼ˆè§£èª¬ãŒæ–‡çŒ® [N] ç¬¬ 8 ç« ã«è©³ã—ã„ï¼‰ã§ã¯è‡ªå‹•å¾®åˆ†ã‚’ç”¨ã„ã‚‹ã“ã¨ã§æ©Ÿæ¢°å­¦ç¿’ã®å®Ÿè·µè€…ã«å„ªã—ã„å†…å®¹ã«ã—ãŸã¨ã„ã†ã“ã¨ã§ã‚ã‚‹ã€‚ã‚ªãƒªã‚¸ãƒŠãƒ«ã¯ TensorFlow ã‚’ç”¨ã„ã¦ã„ã‚‹ãŒã€ä»Šå›ã¯ PyTorch ã‚’ä½¿ã£ã¦ã‚¹ã‚¯ãƒ©ãƒƒãƒã‹ã‚‰å®Ÿè£…ã—ã¦ã¿ãŸã€‚

# ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ãƒ³ã‚½ãƒ«ãƒˆãƒ¬ã‚¤ãƒ³ã®ç¸®ç´„

å„ $\ell \in \{0, \ldots, 9\}$ ã«å¯¾ã™ã‚‹

$$
\begin{align*}
f^{(\ell)}(\bm{x}) = T^{\ell} \cdot \bm{x}
\end{align*}
$$

ã‚’æ˜ã‚Šä¸‹ã’ã‚‹ã€‚å³è¾ºã‚’æ˜ç¤ºçš„ã«æ›¸ãã¨

$$
\begin{align*}
f^{(\ell)}(\bm{x}) = \sum_{i_1,i_2,\ldots,i_6=0}^{1} T_{i_1 i_2 i_3 i_4 i_5 i_6}^{\ell} x_{i_1} x_{i_2} x_{i_3} x_{i_4} x_{i_5} x_{i_6}
\end{align*}
$$

ã¨ãªã‚‹ã€‚å¼ (1) ã‚’æ€ã„å‡ºã™ã¨å…¨ä½“ã¨ã—ã¦ã€â€œè„šâ€ ã«è€ƒãˆã‚‰ã‚Œã‚‹å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã®çµ„ã¿åˆã‚ã›ãŒå®Ÿè¡Œã•ã‚Œã‚‹ã“ã¨ã«ãªã‚‹ã€‚

## æ•°å€¤è¨ˆç®—ä¸Šã®æ‡¸å¿µ

ãƒ†ãƒ³ã‚½ãƒ«ãƒˆãƒ¬ã‚¤ãƒ³ã®å„ãƒãƒ¼ãƒ‰ã‚’ $A$ ã§ä»£è¡¨ã™ã‚‹ã“ã¨ã«ã—ã¦ã€$A$ ã®å„æˆåˆ†ã¨ $F$ ã®å„æˆåˆ†ãŒå‡¡ã $w$ ç¨‹åº¦ã®å€¤ã¨ã™ã‚‹ã€‚ã™ã‚‹ã¨å¼ (1) ã‚ˆã‚Šã€å„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®çµ„ã¿åˆã‚ã›ã”ã¨ã«

$$
\begin{align*}
A^{(1)}_{i_1 \alpha_1} A^{(2)}_{i_2 \alpha_1 \alpha_2} A^{(3)}_{i_3 \alpha_2 \alpha_3} F^{\ell}_{\alpha_3 \alpha_4} A^{(4)}_{i_4 \alpha_4 \alpha_5} A^{(5)}_{i_5 \alpha_5 \alpha_6} A^{(6)}_{i_6 \alpha_6} \approx w^7
\end{align*}
$$

ã¨ãªã‚‹ã€‚$w$ ãŒå¤§ãã„ã¨ç‰¹å¾´é‡ã®å€‹æ•°ã«åˆã‚ã›ã¦æŒ‡æ•°é–¢æ•°çš„ã«å€¤ãŒçˆ†ç™ºã™ã‚‹ã—ã€$w$ ãŒå°ã•ã„ã¨æŒ‡æ•°é–¢æ•°çš„ã«å€¤ãŒæ¶ˆå¤±ã™ã‚‹ã€‚

æ¬¡ã«

$$
\begin{align*}
\sum_{\alpha_1,\alpha_2,\alpha_3,\alpha_4,\alpha_5,\alpha_6=0}^9
\end{align*}
$$

ã®éƒ¨åˆ†ã ãŒã€çµ„ã¿åˆã‚ã›æ•°ã¯ $10^6$ å€‹ã§ã‚ã‚‹ã€‚ã‚ˆã£ã¦ã€$T_{i_1 i_2 i_3 i_4 i_5 i_6}^{\ell} \approx 10^6 w^7$ ã§ã‚ã‚‹ã€‚ã“ã‚ŒãŒç¾å®Ÿå•é¡Œã¨ã—ã¦ã¯ã€`torch.float32` ã®ç¯„å›²ã§æ‰±ã‚ã‚Œã‚‹å¿…è¦ãŒå‡ºã¦ãã‚‹ã€‚

ã¾ãŸã€ç¸®ç´„è¨ˆç®—ä¸­ã« $F^{\ell}_{\alpha_3 \alpha_4}$ ã‚’è¨ˆç®—ã«å«ã‚ã‚‹ã¨ã€$\ell \in \{0, \ldots, 9\}$ ã®ãŸã‚ã€**GPU ãƒ¡ãƒ¢ãƒªä¸Šã«ä¿å­˜ã—ã¦ã„ã‚‹è¨ˆç®—é€”ä¸­ã®ãƒ‡ãƒ¼ã‚¿ãŒ 10 å€ã«ãªã‚‹** ã®ã§ã€ã“ã‚Œã‚’ç¸®ç´„ã™ã‚‹ã®ã¯æœ€å¾Œã«ã™ã‚‹ã»ã†ãŒè‰¯ã„ã€‚ã¤ã¾ã‚Šã€ãƒ†ãƒ³ã‚½ãƒ«ãƒˆãƒ¬ã‚¤ãƒ³ã®ä¸¡ç«¯ã‹ã‚‰å¾ã€…ã«ç¸®ç´„ã—ã¦ã„ã£ã¦ã€æœ€å¾Œã«åˆ†é¡å™¨ã®ãƒãƒ¼ãƒ‰ã¨ã®ç¸®ç´„ã‚’ã¨ã‚‹ã¨ã„ã†å½¢ã ã€‚

FIG. 3. ã«è©³ç´°ãŒæ›¸ã‹ã‚Œã¦ã„ã‚‹ãŒã€ä»Šå›ã¯å°‘ã€…ã•ã¼ã£ã¦ä¸¡ç«¯ã‹ã‚‰ 2 ç‰¹å¾´é‡ãšã¤é–¢é€£ã™ã‚‹ãƒ†ãƒ³ã‚½ãƒ«ã‚’ç¸®ç´„ã—ã¦ã„ãã“ã¨ã«ã—ãŸã€‚

## ãƒ†ãƒ³ã‚½ãƒ«ã®åˆæœŸåŒ–

å®Ÿéš›ã€ä¸Šè¨˜ã«ã‚ˆã†ã«è¨ˆç®—ãŒãƒ‡ãƒªã‚±ãƒ¼ãƒˆãªãŸã‚ã€ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã‚’ç”¨ã„ã¦ã—ã¾ã†ã¨ $w$ ãŒã¨ã¦ã‚‚ã‚·ãƒ“ã‚¢ãªç¯„å›²ã§åˆæœŸåŒ–ã•ã‚Œãªã„ã¨è¨ˆç®—ãŒçˆ†ç™ºã—ãŸã‚Šæ¶ˆå¤± (ï¼Ÿ) ã—ã¦ã—ã¾ã†ã€‚å…¬å¼å®Ÿè£…ã®æ–‡çŒ® [M] ã®ã€Œmnist_example.ipynbã€ã§ã¯ã€Œ**å˜ä½è¡Œåˆ—ã‚’ä¸¦ã¹ãŸã‚ˆã†ãªç–ãªè¡Œåˆ—ã‚’åˆæœŸåŒ–ã®åŸºæœ¬ã¨ã—ã€ãã‚Œã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«åƒ…ã‹ã«æ‘‚å‹•ã™ã‚‹**ã€ã¨ã„ã†æ‰‹æ³•ã‚’ã¨ã£ã¦ã„ã‚‹ã€‚è©¦ã—ãŸé™ã‚Šã“ã‚Œã¯ã†ã¾ãè¡Œãã€‚

ãƒ©ãƒ³ãƒ€ãƒ åˆæœŸåŒ–ã§ã‚‚ã„ã‘ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ãŒã€ã„ã‘ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æ¢ã™ã®ãŒã©ã‚“ã©ã‚“é›£ã—ããªã‚Šã€28x28 ã® MNIST ã§ã¯æ–­å¿µã—ãŸã€‚7x7 ã¾ã§ãƒªã‚µã‚¤ã‚ºã—ãŸ MNIST ã ã¨ã‚ã‚‹ç¨‹åº¦ã¯å­¦ç¿’ã§ãã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¦‹ã¤ã‹ã£ãŸãŒã€ä»Šå›ã¯æ–­å¿µã—ãŸã€‚

# å®Ÿè£…

ä»¥ä¸Šã‚’è¸ã¾ãˆã¦å®Ÿè£…ã—ãŸã„ã€‚ãƒ†ã‚¹ãƒˆã¯ Google Colab ä¸Šã§ T4 ã‚’ç”¨ã„ã¦è¡Œã£ãŸã€‚

## å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã® import

```python
from __future__ import annotations

import math
import os
import sys

import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.optim as optim
import torchinfo
import torchvision
import torchvision.transforms as transforms
from PIL import Image
from torch import nn
from torch.utils.data import DataLoader
```

## MNIST ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

```python
root = os.path.join(os.getenv('HOME'), '.torch')

device = torch.device("cpu")
if torch.cuda.is_available():
    device = torch.device("cuda")

default_pix_dims = 2  # dimension of feature space
default_bond_dims = 10  # common dimension of virtual indices
img_size = 28
n_feature = img_size * img_size

transform = transforms.Compose([
    transforms.ToTensor(),
])

trainset = torchvision.datasets.QMNIST(
    root=root,
    train=True,
    download=True,
    transform=transform
)

testset = torchvision.datasets.QMNIST(
    root=root,
    train=False,
    download=True,
    transform=transform
)
```

## ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã®ä½œæˆ

```python
trainloader = torch.utils.data.DataLoader(
    trainset,
    batch_size = 32,
    shuffle=True,
)

testloader = torch.utils.data.DataLoader(
    testset,
    batch_size = 32,
    shuffle=False,
)
```

## ãƒ¢ãƒ‡ãƒ«ã®å®Ÿè£…

```python
def make_tt(
    n_feature: int, pix_dims: int, bond_dims: int, n_class: int,
    classifier_idx: int | None = None, std: float = 1e-3,
) -> list[torch.Tensor]:
    tt_cores = []
    for i in range(n_feature):
        if i == 0:
            dims = (pix_dims, bond_dims)
            core = torch.zeros(dims)
            core[:, 0] = 1
            core += torch.normal(mean=0.0, std=std, size=core.shape)
        elif i == n_feature - 1:
            dims = (bond_dims, pix_dims)
            core = torch.zeros(dims)
            core[0, :] = 1
            core += torch.normal(mean=0.0, std=std, size=core.shape)
        else:
            dims = (bond_dims, pix_dims, bond_dims)
            core = torch.tensor(
                np.array(pix_dims * [np.eye(bond_dims)],
                dtype=np.float32)
            ).permute(1, 0, 2)
            core += torch.normal(mean=0.0, std=std, size=core.shape)
        tt_cores.append(core)
    if classifier_idx is not None:
        dims = (bond_dims, n_class, bond_dims)
        core = torch.tensor(
            np.array(n_class * [np.eye(bond_dims)], dtype=np.float32)
        ).permute(1, 0, 2)
        core += torch.normal(mean=0.0, std=std, size=core.shape)
        tt_cores.insert(classifier_idx, core)
    return tt_cores


class FeatureEmbeddingLayer(nn.Module):
    def __init__(self):
        super().__init__()

    def forward(self, x: torchTensor) -> tuple[torchTensor, ...]:
        x = torch.flatten(x, start_dim=1)
        x = torch.stack([1-x, x], axis=1).permute((2, 0, 1))
        x = tuple(t.squeeze() for t in x.split(1))
        # n_feature tensors whose shape is (n_batch, pix_dims)
        return x


class WeightLayer(nn.Module):
    def __init__(self, n_feature: int, pix_dims: int, bond_dims: int, n_class: int):
        super().__init__()
        self.n_feature = n_feature
        classifier_idx = self.classifier_loc(n_feature)
        tt_cores = make_tt(
            n_feature, pix_dims, bond_dims, n_class, classifier_idx=classifier_idx
        )
        self.n_cores = len(tt_cores)
        for i, core in enumerate(tt_cores):
            param_core = nn.parameter.Parameter(core)
            setattr(self, f"tt_core{i}", param_core)

    def forward(self, x: tuple[torchTensor, ...], n_sub_features: int = 2):
        classifier_idx = self.classifier_loc(self.n_feature)
        assert(
            n_sub_features * 2 < self.n_feature,
            f"{n_sub_features*2=} must be < {self.n_feature=}"
        )
        n_left_right_block, n_remaining_fea = \
            self.left_sub_feature_num(n_sub_features)

        start = 0
        prev_t = None
        for i in range(n_left_right_block):
            end = start + n_sub_features
            equation = self._make_equation(
                self.n_feature, start_fea=start, end_fea=end
            )
            t = torch.einsum(equation, *x[start:end], *self.tt_cores[start:end])
            if prev_t is None:
                prev_t = t
            else:
                prev_t = torch.einsum("Bb,Bbc->Bc", prev_t, t)

            start += n_sub_features
        left_t = prev_t

        start = n_sub_features * (n_left_right_block * 2 - 1) + n_remaining_fea
        prev_t = None
        for i in range(n_left_right_block):
            end = start + n_sub_features
            equation = self._make_equation(
                self.n_feature, start_fea=start, end_fea=end
            )
            # +1 for tt_cores means consideration of shift for the classifier site
            t = torch.einsum(
                equation, *x[start:end], *self.tt_cores[start + 1:end + 1]
            )
            if prev_t is None:
                prev_t = t
            else:
                prev_t = torch.einsum("Bbc,Bc->Bb", t, prev_t)

            start -= n_sub_features
        right_t = prev_t

        start = n_left_right_block * n_sub_features
        end = start + n_remaining_fea
        equation = self._make_equation(self.n_feature, start_fea=start, end_fea=end)
        # +1 for tt_cores means consideration of including the classifier site
        classifier_t = torch.einsum(
            equation, *x[start:end], *self.tt_cores[start:end + 1]
        )
        output = torch.einsum("ab,abAc,ac->aA", left_t, classifier_t, right_t)
        return output

    def forward_full(self, x: tuple[torchTensor, ...]):
        equation = self._make_equation(self.n_feature)
        output = torch.einsum(equation, *x, *self.tt_cores)
        return output

    def left_sub_feature_num(self, n_sub_features) -> tuple[int, int]:
        remaining = self.n_feature
        i = 0
        while remaining > n_sub_features * 2:
            remaining = remaining - n_sub_features * 2
            i += 1
        return i, remaining

    @property
    def tt_cores(self):
        return [getattr(self, f"tt_core{i}") for i in range(self.n_cores)]

    @classmethod
    def classifier_loc(cls, n_feature):
        return n_feature // 2

    @classmethod
    def _make_equation(
        cls, n_feature: int, start_fea: int = 0, end_fea: int | None = None,
        batch_c: str = "a", class_c: str | None = "A"
    ):
        """make an equation for range(start_i, end_i)
        """

        if start_fea < 0:
            raise ValueError(f'{start_fea=} must be >= 0.')

        if end_fea is None:
            end_fea = n_feature

        if end_fea > n_feature:
            raise ValueError(f'{end_fea=} must be <= {n_feature=}.')

        if start_fea >= end_fea:
            raise ValueError(f'{start_fea=} must be less than {end_fea=}.')

        if end_fea - start_fea > ord("Z") - ord("B") + 1:
            raise ValueError(
                f'{end_fea=} - {start_fea=} must be less than {ord("Z")-ord("B")+1}.'
            )

        classifier_idx = cls.classifier_loc(n_feature)

        includes_classifier = start_fea <= classifier_idx <= end_fea - 1

        fea_i = ord("B")  # U+0042-
        vir_i = ord("b")  # U+0062-

        fea_idx: list[str] = []
        vir_idx: list[str] = []  # ["Bb", "bCc", "cD"]
        pre_vir_idx: list[tuple[str, ...]] = []
        for i in range(start_fea, end_fea):
            fea_idx.append(chr(fea_i))
            if i == 0:
                vir_i -= 1  # preserve first vir_i
                pre_vir_idx.append((chr(fea_i), chr(vir_i + 1)))
            elif i == n_feature - 1:
                pre_vir_idx.append((chr(vir_i), chr(fea_i)))
            else:
                pre_vir_idx.append((chr(vir_i), chr(fea_i), chr(vir_i + 1)))
            fea_i += 1
            vir_i += 1

        classifier_loc = classifier_idx - start_fea
        for i, idx in enumerate(pre_vir_idx):
            if includes_classifier:
                if i == classifier_loc:
                    if i == 0:
                        classifier_idx = f"{idx[0]}{class_c}{chr(ord(idx[0]) + 1)}"
                        vir_idx.append(classifier_idx)
                    else:
                        last_vir_idx = vir_idx[-1][-1]
                        classifier_idx = \
                            f"{last_vir_idx}{class_c}{chr(ord(last_vir_idx) + 1)}"
                        vir_idx.append(classifier_idx)
                if i >= classifier_loc:
                    idx = [c if ord(c) < ord("a") else chr(ord(c) + 1)
                            for c in list(idx)]
            vir_idx.append("".join(idx))

        fea_idx_s = ",".join([f"{batch_c}{c}"for c in fea_idx])

        out_index = batch_c
        vir_idx_s = ",".join(vir_idx)
        if ord(vir_idx_s[0]) >= ord("a"):
            out_index += vir_idx_s[0]
        if class_c in vir_idx_s:
            out_index += class_c
        if ord(vir_idx_s[-1]) >= ord("a"):
            out_index += vir_idx_s[-1]

        equation = f'{fea_idx_s + "," + vir_idx_s}->{out_index}'

        return equation


class Net(nn.Module):
    def __init__(self, n_feature: int, pix_dims: int, bond_dims: int, n_class: int):
        super().__init__()

        self.fea_layer = FeatureEmbeddingLayer()
        self.wgt_layer = WeightLayer(
            n_feature=n_feature, pix_dims=pix_dims, bond_dims=bond_dims,
            n_class=n_class
        )
        self.softmax = nn.LogSoftmax(dim=1)

    def forward(self, x):
        x = self.fea_layer(x)
        x = self.wgt_layer(x)
        x = self.softmax(x)
        return x
```

## è¨“ç·´ã¨æ¤œè¨¼ãƒ«ãƒ¼ãƒ—ã®å®Ÿè£…

```python
def train(net, device, train_loader, optimizer, epoch, log_interval):
    losses = []
    nll_loss = nn.NLLLoss()

    net.train()
    running_loss = 0
    n_samples = 0
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = net(data)
        loss = nll_loss(output, target)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()
        n_samples += len(data)
        if batch_idx % log_interval == 0:
            losses.append(running_loss / n_samples)
            running_loss = 0
            n_samples = 0
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item() / len(data)))
    return losses


def test(net, device, test_loader):
    nll_loss = nn.NLLLoss()

    net.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = net(data)
            test_loss += nll_loss(output, target).item()  # sum up batch loss
            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader.dataset)

    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))
```

## ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆã¨è¨“ç·´ã¨æ¤œè¨¼

```python
%%time

net = Net(n_feature=n_feature, pix_dims=default_pix_dims,
          bond_dims=default_bond_dims, n_class=10).to(device)

optimizer = optim.Adam(net.parameters())

log_interval = 50
epochs = 1

losses = []
for epoch in range(1, epochs + 1):
    sublosses = train(net, device, trainloader, optimizer, epoch, log_interval)
    losses += sublosses
    test(net, device, testloader)
```

> Train Epoch: 1 [0/60000 (0%)]	Loss: 0.071956
> Train Epoch: 1 [1600/60000 (3%)]	Loss: 0.071391
> Train Epoch: 1 [3200/60000 (5%)]	Loss: 0.071370
> Train Epoch: 1 [4800/60000 (8%)]	Loss: 0.033660
> Train Epoch: 1 [6400/60000 (11%)]	Loss: 0.045731
> Train Epoch: 1 [8000/60000 (13%)]	Loss: 0.028915
> Train Epoch: 1 [9600/60000 (16%)]	Loss: 0.029875
> Train Epoch: 1 [11200/60000 (19%)]	Loss: 0.032353
> Train Epoch: 1 [12800/60000 (21%)]	Loss: 0.018047
> Train Epoch: 1 [14400/60000 (24%)]	Loss: 0.023287
> Train Epoch: 1 [16000/60000 (27%)]	Loss: 0.022523
> Train Epoch: 1 [17600/60000 (29%)]	Loss: 0.026113
> Train Epoch: 1 [19200/60000 (32%)]	Loss: 0.015155
> Train Epoch: 1 [20800/60000 (35%)]	Loss: 0.017743
> Train Epoch: 1 [22400/60000 (37%)]	Loss: 0.023616
> Train Epoch: 1 [24000/60000 (40%)]	Loss: 0.020549
> Train Epoch: 1 [25600/60000 (43%)]	Loss: 0.021021
> Train Epoch: 1 [27200/60000 (45%)]	Loss: 0.019376
> Train Epoch: 1 [28800/60000 (48%)]	Loss: 0.020880
> Train Epoch: 1 [30400/60000 (51%)]	Loss: 0.030461
> Train Epoch: 1 [32000/60000 (53%)]	Loss: 0.037976
> Train Epoch: 1 [33600/60000 (56%)]	Loss: 0.013896
> Train Epoch: 1 [35200/60000 (59%)]	Loss: 0.006066
> Train Epoch: 1 [36800/60000 (61%)]	Loss: 0.009239
> Train Epoch: 1 [38400/60000 (64%)]	Loss: 0.017888
> Train Epoch: 1 [40000/60000 (67%)]	Loss: 0.003709
> Train Epoch: 1 [41600/60000 (69%)]	Loss: 0.031147
> Train Epoch: 1 [43200/60000 (72%)]	Loss: 0.023640
> Train Epoch: 1 [44800/60000 (75%)]	Loss: 0.005690
> Train Epoch: 1 [46400/60000 (77%)]	Loss: 0.012589
> Train Epoch: 1 [48000/60000 (80%)]	Loss: 0.007181
> Train Epoch: 1 [49600/60000 (83%)]	Loss: 0.014580
> Train Epoch: 1 [51200/60000 (85%)]	Loss: 0.007948
> Train Epoch: 1 [52800/60000 (88%)]	Loss: 0.012705
> Train Epoch: 1 [54400/60000 (91%)]	Loss: 0.020037
> Train Epoch: 1 [56000/60000 (93%)]	Loss: 0.013667
> Train Epoch: 1 [57600/60000 (96%)]	Loss: 0.023198
> Train Epoch: 1 [59200/60000 (99%)]	Loss: 0.015644
>
> Test set: Average loss: 0.0131, Accuracy: 54402/60000 (90.67%)
>
> CPU times: user 47min 25s, sys: 5.34 s, total: 47min 31s
> Wall time: 47min 37s

## æå¤±ã®æ¨ç§»

```python
plt.plot(losses)
plt.show()
```

![](/images/dwd-matrix-product12/003.png =400x)

# é©šãã®é«˜æ¬¡å…ƒ

$28 \times 28 = 784$ ã®ç”»ç´ ãƒ‡ãƒ¼ã‚¿ã‚’ç‰¹å¾´ãƒãƒƒãƒ—ã§ãƒ†ãƒ³ã‚½ãƒ«ç©ã«ã†ã¤ã—ãŸã®ã§ã€æ¦‚å¿µã¨ã—ã¦ã¯ $2^{784} =$ 101745825697019260773923519755878567461315282017759829107608914364075275235254395622580447400994175578963163918967182013639660669771108475957692810857098847138903161308502419410142185759152435680068435915159402496058513611411689167650816 æ¬¡å…ƒã® Hilbert ç©ºé–“ã®ä¸­ã§ã®åˆ†é¡ã‚’è¡Œã£ãŸãƒ»ãƒ»ãƒ»ã¿ãŸã„ãªçŠ¶æ…‹ã«ãªã£ã¦ã„ã‚‹ã®ã§ã¯ãªã„ã‹ã¨æ€ã†ãŒã€ãã†ã„ã†è¦‹æ–¹ã‚‚ã§ããã†ã§ã¯ã‚ã‚‹ãŒã€ç‰©å‡„ã„å¤§è¢ˆè£Ÿæ„Ÿã¯ã‚ã‚‹ã€‚

# ã¾ã¨ã‚

ã¨ã‚Šã‚ãˆãšã€ä½•ã¨ã‹è«–æ–‡ãŒä¸»å¼µã™ã‚‹ã‚ˆã†ãªç²¾åº¦ãŒå‡ºã‚‹å®Ÿè£…ãŒã§ãã¦è‰¯ã‹ã£ãŸã€‚

ã¨ã“ã‚ã§è«–æ–‡ã«ã‚ˆã‚‹ã¨

> _The purpose of this note is to be a resource for machine learning practitioners who wish to learn how tensor networks can be applied to classification problems._

ã¨ã„ã†ã“ã¨ãªã®ã ãŒã€_machine learning practitioners_ ã®ç«‹å ´ã¨ã—ã¦ã¯ã¡ã‚‡ã£ã¨ãã¤ã‹ã£ãŸãƒ»ãƒ»ãƒ»ã€‚çµæ§‹ç°¡å˜ã«ä½¿ãˆã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨åˆæœŸåŒ–æ‰‹æ³•ã€ãã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç­‰ã€…ãŒç”¨æ„ã•ã‚Œã¦ã„ãªã„ã¨ã€è‡ªåˆ†ã§å®Ÿè£…ã™ã‚‹ã®ã¯å¤§å¤‰ã ãªã¨æ€ã£ãŸã€‚å‹¿è«– [TensorNetwork](https://github.com/google/TensorNetwork) ãŒã‚ã‚‹ã«ã¯ã‚ã‚‹ãŒã€ãŸã ã® _machine learning practitioners_ ãŒä½¿ã„ã“ãªã™ã«ã¯ **ãƒ†ãƒ³ã‚½ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ¦‚å¿µã‚’å«ã‚ã¦** ãƒãƒ¼ãƒ‰ãƒ«ãŒé«˜ã„ãªãƒ»ãƒ»ãƒ»ã¨æ„Ÿã˜ã‚‹ã‚ã‘ã§ã‚ã‚‹ã€‚

ã©ã¡ã‚‰ã‹ã¨è¨€ã†ã¨ [è¡Œåˆ—ç©çŠ¶æ…‹ã«ã¤ã„ã¦è€ƒãˆã‚‹ (5) â€” ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ¢ãƒ‡ãƒ«åœ§ç¸®](https://zenn.dev/derwind/articles/dwd-matrix-product05) ã®ã‚ˆã†ã«ã€æ—¢ã«è¨“ç·´ãŒå®Œäº†ã—ã¦ã„ã‚‹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ãƒ†ãƒ³ã‚½ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«å¤‰æ›ã—ã¦å¾®èª¿æ•´ã‚’è¡Œã†ã¨ã„ã£ãŸç”¨é€”ã®ã»ã†ãŒæ‰±ã„ã‚‚ç°¡å˜ã ã¨æ„Ÿã˜ã¦ã„ã¦ã€ã‚¹ã‚¯ãƒ©ãƒƒãƒã§ãƒ†ãƒ³ã‚½ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’è¨“ç·´ã™ã‚‹ã®ã¯ãã¤ã„ãªã¨æ„Ÿã˜ã‚‹ã®ã ãŒã©ã†ãªã®ã ã‚ã†ã‹ã€‚

# å‚è€ƒæ–‡çŒ®

[E] [TensorNetwork for Machine Learning, arXiv:1906.06329, Stavros Efthymiou, Jack Hidary, Stefan Leichenauer](https://arxiv.org/abs/1906.06329)
[M] [MPS_classifier@TensorNetwork, GitHub, Google LLC](https://github.com/google/TensorNetwork/tree/0.1.0/experiments/MPS_classifier)
[S] [Supervised Learning with Quantum-Inspired Tensor Networks, arXiv:1605.05775, E. Miles Stoudenmire, David J. Schwab](https://arxiv.org/abs/1605.05775)
[N] [è¥¿é‡å‹å¹´, ãƒ†ãƒ³ã‚½ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å…¥é–€, è¬›è«‡ç¤¾, 2023](https://www.kspub.co.jp/book/detail/5316535.html)
