---
title: "Qiskit ã§éŠã‚“ã§ã¿ã‚‹ (15) â€” Quantum Machine Learning"
emoji: "ğŸª"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["Qiskit", "Python", "é‡å­æ©Ÿæ¢°å­¦ç¿’", "æ©Ÿæ¢°å­¦ç¿’"]
published: true
---

# ç›®çš„

Edward Grant et al. ã® [Hierarchical quantum classifiers](https://www.nature.com/articles/s41534-018-0116-9) (arXiv:1804.03680) ã¨ã„ã†è«–æ–‡ãŒã‚ã£ã¦ã€é¢ç™½ãã†ãªã®ã§å®Ÿè£…ã—ã¦è©¦ã—ã¦ã¿ã‚ˆã†ã¨ã„ã†å†…å®¹ã€‚

ä»Šå›ã¯ãã®ä¸­ã®äºˆå‚™å®Ÿé¨“ã«ã‚ãŸã‚‹ï¼Ÿ Iris ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ã® Tree TensorNetwork Classifier ã‚’è©¦ã™[^1]ã€‚

[^1]: æ—¢ã« MNIST ã®ã»ã†ã‚‚è©¦ã—ã¦ã„ã‚‹ãŒã€å†…å®¹çš„ã«åŒæ§˜ã«ã‚„ã‚Œã°ã¡ã‚ƒã‚“ã¨çµæœãŒå‡ºã‚‹ã€‚

# æ¦‚è¦

å°‘ã€…èª­ã¿é•ãˆã¦ã„ã‚‹ã¨ã“ã‚ã‚‚ã‚ã‚‹ã‹ã‚‚ã—ã‚Œãªã„ãŒã€ä»¥ä¸‹ã®è¨­å®šã¨ã—ãŸã€‚

- Iris ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ (4 æ¬¡å…ƒ)ã€æˆ–ã¯ MNIST (784 æ¬¡å…ƒ) ã‚’é‡å­å›è·¯ã«æŒ¯å¹…ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹ã€‚MNIST ã®å ´åˆã¯æ¬¡å…ƒãŒå¤§ãã„ã®ã§ã€ä¸»æˆåˆ†åˆ†æ (PCA) ã‚’ç”¨ã„ã¦ 8 æ¬¡å…ƒã«è½ã¨ã™ã€‚
- é‡å­å›è·¯ã¯ TTN (Tree TensorNetwork) ã‹ MERA (Multi-scale entanglement renormalization) ã®æ§‹é€ ã‚’ç”¨ã„ã‚‹ã€‚
- ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ $I \otimes \cdots I \otimes Z \otimes I \otimes \cdots I$ ã®æœŸå¾…å€¤ $\{-1, 1\}$ ã®æ¸¬å®šã«ã‚ˆã‚‹äºŒå€¤åˆ†é¡ã§ã‚ã‚‹ã€‚
- å‹¾é…ã‚’æ¨å®šã—ã€Adam ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã§ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã™ã‚‹ç¢ºç‡çš„å‹¾é…é™ä¸‹ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ç”¨ã„ã‚‹ã€‚

å…·ä½“çš„ãªå›è·¯ã®æ§‹é€ ã¯ã€MNIST ã«ã¤ã„ã¦ã¯ Figure 1 ã‚’ã€Iris ã«ã¤ã„ã¦ã¯ Figure 6 ã‚’è¦‹ã‚Œã°è‰¯ã„ã€‚

# å®Ÿé¨“ç’°å¢ƒ

```sh
$ pip list | grep qiskit
qiskit                   0.44.0
qiskit-aer-gpu-cu11      0.12.2
qiskit-dynamics          0.4.1
qiskit-experiments       0.5.3
qiskit-finance           0.3.4
qiskit-ibm-experiment    0.2.7
qiskit-ibmq-provider     0.20.2
qiskit-machine-learning  0.6.1
qiskit-nature            0.6.2
qiskit-optimization      0.5.0
qiskit-terra             0.25.0

$ pip list | grep torch
torch                    2.0.1+cu118
torchaudio               2.0.2+cu118
torchmetrics             0.11.1
torchvision              0.15.2+cu118
```

# æº–å‚™

å¿…è¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’ç”¨æ„ã—ã¦ã„ãã€‚ç‰¹ã« QML ã«åˆ©ç”¨ã§ãã‚‹ Adam ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã‚’èª¿é”ã™ã‚‹ã€‚

## ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶

1 ã¤ã®å€™è£œã¯ [numpy-ml](https://pypi.org/project/numpy-ml/) ã§ã€Python 3.9 ã¾ã§ã¯ AS IS ã§åˆ©ç”¨ã§ãã‚‹ãŒã€Python 3.10 ã§ã¯ 1 ç®‡æ‰€ä¿®æ­£ã—ãªã„ã¨ãªã‚‰ãªã„ã€‚ã‚‚ã† 1 ã¤ã®å€™è£œã¯ [dezero](https://pypi.org/project/dezero/) ã§ã€ã“ã‚Œã¯æœ¬æ¥ã¯ã€Œ[ã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹Deep Learning â¸](https://www.oreilly.co.jp/books/9784873119069/)ã€ã§ä½œæˆã™ã‚‹ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã€ŒDeZeroã€ã§ã‚ã‚‹ã€‚

ä»Šå›ã¯å¾Œè€…ã‚’æ¡ç”¨ã—ãŸã€‚Adam ã ã‘æ¬²ã—ã„ã®ã§å±€æ‰€çš„ã«åˆ‡ã‚Šå‡ºã›ã‚‹ãŸã‚ã§ã‚ã‚‹ã€‚ã‚³ãƒ¼ãƒ‰ã¯ MIT ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã§æä¾›ã•ã‚Œã¦ãŠã‚Šéå¸¸ã«ä½¿ã„ã‚„ã™ã„ã€‚[deep-learning-from-scratch-3/dezero/optimizers.py](https://github.com/oreilly-japan/deep-learning-from-scratch-3/blob/master/dezero/optimizers.py) ã‚’æ”¹é€ ã—ã¦ä»¥ä¸‹ã®ã‚ˆã†ã«ã—ãŸ:

```python
from __future__ import annotations

import math
import numpy as np


# =============================================================================
# Optimizer (base class)
# =============================================================================
class Optimizer:
    def __init__(self):
        self.hooks = []

    def update(self, params, grads) -> None:
        for f in self.hooks:
            f(params)

        self.update_one(params, grads)

    def update_one(self, params, grads) -> None:
        raise NotImplementedError()

    def add_hook(self, f):
        self.hooks.append(f)


# =============================================================================
# SGD / MomentumSGD / AdaGrad / AdaDelta / Adam
# =============================================================================
class Adam(Optimizer):
    def __init__(self, alpha=0.001, beta1=0.9, beta2=0.999, eps=1e-8):
        super().__init__()
        self.t = 0
        self.alpha = alpha
        self.beta1 = beta1
        self.beta2 = beta2
        self.eps = eps
        self.ms = {}
        self.vs = {}

    def update(self, *args, **kwargs):
        self.t += 1
        super().update(*args, **kwargs)

    @property
    def lr(self):
        fix1 = 1. - math.pow(self.beta1, self.t)
        fix2 = 1. - math.pow(self.beta2, self.t)
        return self.alpha * math.sqrt(fix2) / fix1

    def update_one(self, params, grads):
        key = 'Adam'
        if key not in self.ms:
            self.ms[key] = self.np.zeros_like(params)
            self.vs[key] = self.np.zeros_like(params)

        m, v = self.ms[key], self.vs[key]
        beta1, beta2, eps = self.beta1, self.beta2, self.eps
        grad = grads

        m += (1 - beta1) * (grad - m)
        v += (1 - beta2) * (grad * grad - v)
        params -= self.lr * m / (self.np.sqrt(v) + eps)
```

## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€

Iris ã ã‘ãªã‚‰ã©ã†ã„ã†å®Ÿè£…ã§ã‚‚è‰¯ã„ã®ã ãŒã€è‰²ã‚“ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’æ‰±ã†å ´åˆã€Torchvision ã¯ä¾¿åˆ©ã§ã‚ã‚‹ã€‚ã¨ãªã‚Œã°ã€ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã¨ã—ã¦ PyTorch ã®ã‚‚ã®ã«ä¹—ã›ã‚‹ã¨å¾Œã€…æ¥½ãŒã§ããã†ã§ã‚ã‚‹ã€‚ã¨ã„ã†ã“ã¨ã§ä»¥ä¸‹ã®ã‚ˆã†ãªä»•çµ„ã¿ã‚’ç”¨æ„ã™ã‚‹ã€‚ãƒ†ãƒ³ã‚½ãƒ«ã®å¤‰æ›ã‚’é©ç”¨ã§ãã‚‹ã‚ˆã†ãª `TersorDataset` ãŒæ¬²ã—ã„ã¨ã„ã†å†…å®¹ã«ã¤ã„ã¦ã€ãƒ•ã‚©ãƒ¼ãƒ©ãƒ  [Transforms for TensorDataset()](https://discuss.pytorch.org/t/transforms-for-tensordataset/178645/2) ã«è‰¯ã„ç­”ãˆãŒæ›¸ã‹ã‚Œã¦ã„ãŸã®ã§ã“ã‚Œã‚‚å‚è€ƒã«ã—ãŸã€‚

```python
class SimpleToTensor:
    def __init__(self, dtype=int):
        self.dtype=dtype

    def __call__(self, x):
        return torch.tensor(x, dtype=self.dtype)

    def __repr__(self) -> str:
        return f"{self.__class__.__name__}()"

def transform_label(label):
    return 1 if label == 1 else -1

target_transform = transforms.Compose([
    SimpleToTensor(),
    transform_label
])

class TransformableDataset(Dataset):
    def __init__(self,data, target, transform=None, target_transform=None):
        self.data = data
        self.target = target
        self.transform = transform
        self.target_transform = target_transform
        
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, index):
        x = self.data[index]
        if self.transform:
            x = self.transform(x)
        
        y = self.target[index]
        if self.target_transform:
            y = self.target_transform(y)
            
        return x, y
```

`transform_label` ã®å†…å®¹ã¯æš—é»™ã®ã†ã¡ã«ã€Versicolor ã¨ãã®ä»–ã§ã®äºŒå€¤åˆ†é¡ã‚’æƒ³å®šã—ã¦ã„ã‚‹ã€‚Setosa (0), Sersicolor (1), Versinica (2) ã§ã€ç‰¹ã«ä»Šå›ã¯ Sersicolor (1) vs Versinica (2) ã‚’ã™ã‚‹ã®ã§ã€**Versinica ã‚’ -1 ã«ãªã‚‹ã‚ˆã†ã«ã—ã¦ã„ã‚‹**ã€‚ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã¯ PyTorch ã®ã‚‚ã®ã‚’ AS IS ã§ä½¿ã†ã“ã¨ã«ã™ã‚‹ã€‚

# ãƒ¡ã‚¤ãƒ³å®Ÿè£…

æº–å‚™ãŒã§ããŸã®ã§ãƒ¡ã‚¤ãƒ³ã®å®Ÿè£…ã‚’é€²ã‚ã‚‹ã€‚åŸºæœ¬çš„ã«ã¯æ·±å±¤å­¦ç¿’ã®ãã‚Œã¨åŒã˜ã§ã‚ã‚‹ã€‚

åŸºæœ¬çš„ã«ã¯è«–æ–‡ã®é€šã‚Šã«å®Ÿè£…ã™ã‚‹ã®ã§ã€Qiskit ã®è¨€è‘‰ã«ç¿»è¨³ã™ã‚‹ç¨‹åº¦ã§ã‚ã‚‹ã€‚

## å¿…è¦ãªãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ import ã™ã‚‹

```python
from __future__ import annotations

import os
import sys
import math
import pickle
from collections.abc import Sequence
from sklearn import datasets
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import DataLoader, Dataset, TensorDataset
import torchvision
import torchvision.transforms as transforms
import numpy as np
import matplotlib.pyplot as plt

sys.path.append('.')
import common.optimizers as optimizers

from qiskit import QuantumCircuit
from qiskit.circuit import ParameterVector
from qiskit.quantum_info import SparsePauliOp
from qiskit.primitives import BaseEstimator, Estimator
from qiskit.quantum_info.operators.base_operator import BaseOperator
from qiskit_machine_learning.neural_networks import EstimatorQNN
```

## Versicolor ã¨ Versinica ã®ã¿ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œã‚‹

```python
iris = datasets.load_iris()

# Sersicolor (1) ã¨ Versinica (2)
indices = np.where((iris.target==1) | (iris.target==2))
versicolor_versinica_data = np.squeeze(iris.data[indices, :], axis=0)
versicolor_versinica_target = iris.target[indices]

X_train, X_test, y_train, y_test = train_test_split(
    versicolor_versinica_data, versicolor_versinica_target,
    test_size=0.3, random_state=1234
)

trainset = TransformableDataset(
    X_train, y_train, SimpleToTensor(float), target_transform
)

testset = TransformableDataset(
    X_test, y_test, SimpleToTensor(float), target_transform
)
```

## é‡å­åˆ†é¡å™¨ã‚’å®Ÿè£…ã™ã‚‹

è«–æ–‡ã® Figure 1, 2, 6 ã«æ³¨æ„ã—ã¦å®Ÿè£…ã™ã‚‹ã€‚

```python
n_qubits = 4

# Eqn. (1)
def make_init_circuit(
    n_qubits: int,
    dry_run: bool = False
) -> QuantumCircuit | int:
    if dry_run:
        return n_qubits

    init_circuit = QuantumCircuit(n_qubits)
    x = ParameterVector('x', n_qubits)
    for i in range(n_qubits):
        init_circuit.ry(x[i], i)

    return init_circuit

# Fig. 1 (a) TTN classifier
def make_ansatz(
    n_qubits: int,
    insert_barrier: bool = False,
    dry_run: bool = False
) -> QuantumCircuit | int:
    def append_U(qc, i, j, thetas, count, last_unitary=False, reverse=False):
        qc.ry(thetas[count], i)
        count += 1
        qc.ry(thetas[count], j)
        count += 1

        if reverse:
            ansatz.cx(j, i)
        else:
            ansatz.cx(i, j)
        if last_unitary:
            qc.ry(thetas[count], j)
            count += 1
        return count

    length = 2*n_qubits//2  # U5 - U6
    length += 3*n_qubits//4  # U7

    if dry_run:
        return length

    thetas = ParameterVector('Î¸', length)

    count = 0
    ansatz = QuantumCircuit(n_qubits)
    # U5 - U6
    reverse = False
    for i in range(0, n_qubits, 2):
        if i+1 >= n_qubits:
            break
        count = append_U(ansatz, i, i+1, thetas, count, reverse=reverse)
        reverse = not reverse
    if insert_barrier:
        ansatz.barrier()
    # U7
    for i in range(1, n_qubits, 4):
        if i+1 >= n_qubits:
            break
        count = append_U(ansatz, i, i+1, thetas, count, last_unitary=True)
    if insert_barrier:
        ansatz.barrier()
    assert count == length, count
    return ansatz

def make_placeholder_circuit(
    n_qubits: int,
    insert_barrier: bool = False,
    dry_run: bool = False
) -> QuantumCircuit | int:
    if dry_run:
        length_feature = make_init_circuit(n_qubits, dry_run=True)
        length_ansatz = make_ansatz(n_qubits, dry_run=True)
        length = length_feature + length_ansatz
        return length

    qc = make_init_circuit(n_qubits)
    ansatz = make_ansatz(n_qubits, insert_barrier)
    qc.compose(ansatz, inplace=True)

    return qc

placeholder_circuit = make_placeholder_circuit(n_qubits)
display(placeholder_circuit.draw())
```

![](/images/dwd-qiskit15/001.png)

## æœŸå¾…å€¤ã‚’æ¨å®šã™ã‚‹ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã‚’å®šç¾©ã™ã‚‹

ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ãŒè«–æ–‡ã«ã¯å…·ä½“çš„ã«ã¯æ›¸ã„ã¦ã„ãªã„ãŒã€è¦³æ¸¬é‡ $M$ ã«é–¢ã™ã‚‹èª¬æ˜ a simple Pauli measurement ã‹ã‚‰ Z æ¸¬å®šã«å¯¾å¿œã™ã‚‹ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ã¨ã„ã†ã“ã¨ã«ã—ãŸã€‚ã¤ã¾ã‚Šã€$\braket{M} = P(\ket{0}) - P(\ket{1})$ ã¨ã„ã†æ„Ÿã˜ã ã€‚

```python
hamiltonian = SparsePauliOp('IZII')  # 3rd position from the right, c.f. Fig. 1
```

## è¨“ç·´ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè£…ã™ã‚‹

åŸºæœ¬çš„ã« PyTorch ã¿ãŸã„ãªæ„Ÿã˜ã«ãªã‚‹ã€‚[EstimatorQNN](https://qiskit.org/ecosystem/machine-learning/stubs/qiskit_machine_learning.neural_networks.EstimatorQNN.html) ã®èª¬æ˜

> - gradient (_BaseEstimatorGradient_ | _None_) â€“ The estimator gradient to be used for the backward pass. If None, a default instance of the estimator gradient, [ParamShiftEstimatorGradient](https://qiskit.org/documentation/stubs/qiskit.algorithms.gradients.ParamShiftEstimatorGradient.html#qiskit.algorithms.gradients.ParamShiftEstimatorGradient), will be used.

ã«ã‚ã‚‹ã‚ˆã†ã«ã€ç‰¹ã«æŒ‡å®šã—ãªã‘ã‚Œã°å‹¾é…è¨ˆç®—ã¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚·ãƒ•ãƒˆå‰‡ãŒä½¿ã‚ã‚Œã‚‹ã€‚

ãªãŠã€[def _backward](https://github.com/qiskit-community/qiskit-machine-learning/blob/0.6.1/qiskit_machine_learning/neural_networks/estimator_qnn.py#L219-L251) ã®å†…å®¹ã‚ˆã‚Šã€ã‚³ã‚¹ãƒˆé–¢æ•°ã¨ã—ã¦ã®å‹¾é…è¨ˆç®—ã§ã¯ãªãã€$\nabla\braket{\psi(\theta) | H | \psi(\theta)}$ ã®ã¿è¨ˆç®—ã™ã‚‹ã‚ˆã†ãªã®ã§ã€ã‚³ã‚¹ãƒˆé–¢æ•°ã®å‹¾é…ã¯åˆ¥é€”çµ„ã¿ç«‹ã¦ã‚‹å¿…è¦ãŒã‚ã‚‹ã€‚ã“ã‚Œã«ã¤ã„ã¦ã¯ Appendix ã§ç°¡å˜ã«è§¦ã‚Œã‚‹ã€‚

```python
class PQCTrainerEstimatorQnn:
    def __init__(self,
        qc: QuantumCircuit,
        initial_point: Sequence[float],
        optimizer: optimizers.Optimizer,
        estimator: BaseEstimator | None = None
    ):
        self.qc_pl = qc  # placeholder circuit
        self.initial_point = np.array(initial_point)
        self.optimizer = optimizer
        self.estimator = estimator

    def fit(self,
        dataset: Dataset,
        batch_size: int,
        operator: BaseOperator,
        callbacks: list | None = None,
        epochs: int = 1
    ):
        dataloader = DataLoader(dataset, batch_size, shuffle=True, drop_last=True)
        callbacks = callbacks if callbacks is not None else []

        opt_loss = sys.maxsize
        opt_params = None
        params = self.initial_point.copy()

        n_qubits = self.qc_pl.num_qubits
        qnn = EstimatorQNN(
            circuit=self.qc_pl, estimator=self.estimator, observables=operator,
            input_params=self.qc_pl.parameters[:n_qubits],
            weight_params=self.qc_pl.parameters[n_qubits:]
        )
        print(f'num_inputs={qnn.num_inputs}')

        for epoch in range(epochs):
            for batch, label in dataloader:
                batch, label = self._preprocess_batch(batch, label)
                label = label.reshape(label.shape[0], -1)

                expvals = qnn.forward(input_data=batch, weights=params)
                total_loss = np.mean((expvals - label)**2)

                _, grads = qnn.backward(input_data=batch, weights=params)
                grads = np.squeeze(grads, axis=1)
                # ã‚³ã‚¹ãƒˆé–¢æ•°ã®å‹¾é…ã‚’çµ„ã¿ç«‹ã¦ã¦ã€ãƒãƒƒãƒã§ã®å¹³å‡ã‚’ã¨ã‚‹ã€‚
                total_grads = np.mean((expvals - label) * grads, axis=0)

                if total_loss < opt_loss:
                    opt_params = params.copy()
                    opt_loss = total_loss

                    with open('opt_params_iris.pkl', 'wb') as fout:
                        pickle.dump(opt_params, fout)

                self.optimizer.update(params, total_grads)

                for callback in callbacks:
                    callback(total_loss, params)

        return opt_params

    def _preprocess_batch(self,
        batch: torch.Tensor,
        label: torch.Tensor
    ) -> tuple[np.ndarray, np.ndarray]:
        batch = batch.detach().numpy()
        label = label.detach().numpy()
        return batch, label


def RunPQCTrain(
    dataset: Dataset,
    batch_size: int,
    qc: QuantumCircuit,
    operator: BaseOperator,
    init: Sequence[float] | None = None,
    estimator: Estimator | None = None,
    epochs: int = 1,
    interval = 100
):    
    # Store intermediate results
    history = {'loss': [], 'params': []}
    cnt = 0

    def store_intermediate_result(loss, params):
        nonlocal cnt
        if cnt % interval != 0:
            return
        history['loss'].append(loss)
        history['params'].append(None)  # ã¨ã‚Šã‚ãˆãšä¿å­˜ã—ãªã„ã“ã¨ã«ã™ã‚‹ã€‚
        print(f'{loss=}')

    # alpha ã®å€¤ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚ˆã‚Šå¤§ãã„ã»ã†ãŒåæŸãŒæ—©ã‹ã£ãŸã€‚
    optimizer = optimizers.Adam(alpha=0.01)
    trainer = PQCTrainerEstimatorQnn(
        estimator=estimator, qc=qc, initial_point=init, optimizer=optimizer
    )
    result = trainer.fit(
        dataset, batch_size, operator,
        callbacks=[store_intermediate_result], epochs=epochs
    )

    return result, history['loss']
```

å®Ÿè£…ã¯ã“ã“ã¾ã§ã§ã‚ã‚‹ã€‚

# å®Ÿé¨“ã‚’è¡Œã†

ãƒãƒƒãƒã‚µã‚¤ã‚º 32 ã§ 100 ã‚¨ãƒãƒƒã‚¯å›ã™ã€‚500 å›ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ›´æ–°ã”ã¨ã«ã‚³ã‚¹ãƒˆå€¤ã‚’å‡ºã™ã€‚

```python
length = make_ansatz(n_qubits, dry_run=True)
placeholder_circuit = make_placeholder_circuit(n_qubits)

np.random.seed(10)
init = np.random.random(length) * 2*math.pi

estimator = Estimator()
opt_params, loss_list = RunPQCTrain(
    trainset, 32,
    placeholder_circuit, hamiltonian, init=init, estimator=estimator,
    epochs=100, interval=500)

print(f'final loss={loss_list[-1]}')
print(f'{opt_params=}')
```

# çµæœã‚’æ¤œè¨¼ã™ã‚‹

## ã‚³ã‚¹ãƒˆå€¤ã®ãƒ—ãƒ­ãƒƒãƒˆ

ã‚³ã‚¹ãƒˆã®å¤‰åŒ–ã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ã¨ä»¥ä¸‹ã®ã‚ˆã†ã«å¾ã€…ã«ä¸‹ãŒã£ã¦ã„ãã“ã¨ãŒåˆ†ã‹ã‚‹ã€‚ã‚ã‚‹ç¨‹åº¦ä»¥ä¸Šã¯ä¸‹ãŒã‚‰ãªã„ã®ã§ã€ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ãŒå°‘ãªã„ã®ã‹ã‚‚ã—ã‚Œãªã„ã€‚

```python
plt.plot(range(len(loss_list)), loss_list)
plt.show()
```

![](/images/dwd-qiskit15/002.png)

## ãƒ†ã‚¹ãƒˆç²¾åº¦

ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å‡ºåŠ›ã¯ [-1, 1] ã®ç¯„å›²ãªã®ã§ã€0 ä»¥ä¸Šã‚’ 1ã€0 æœªæº€ã‚’ -1 ã®ãƒ©ãƒ™ãƒ«ã«å¯¾å¿œä»˜ã‘ã¦ã€æ­£è§£ãƒ©ãƒ™ãƒ«ã¨æ¯”è¼ƒã™ã‚‹ã€‚

```python
testloader = DataLoader(testset, 32)

qc_pl = make_placeholder_circuit(n_qubits)
estimator = Estimator()

total = 0
total_correct = 0

for i, (batch, label) in enumerate(testloader):
    batch, label = batch.detach().numpy(), label.detach().numpy()

    qc_list = []

    for i in range(batch.shape[0]):
        data = batch[i, :]
    
        qc_placeholder = qc_pl.copy()
        qc = qc_placeholder.bind_parameters(data.tolist() + params.tolist())
        qc_list.append(qc)

    job = estimator.run(qc_list, [hamiltonian]*len(qc_list))
    result = job.result()
    expvals = result.values

    predict_labels = np.ones_like(expvals)
    predict_labels[np.where(expvals < 0)] = -1
    predict_labels = predict_labels.astype(int)

    total_correct += np.sum(predict_labels == label)
    total += batch.shape[0]

print(f'test acc={np.round(total_correct/total, 2)}')
```

> 0.9

ä½•åº¦ã‹è©¦ã—ãŸãŒã€30 å€‹ã®ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ã†ã¡ã€27 å€‹ç¨‹åº¦ãŒæ­£ã—ãåˆ†é¡ã•ã‚Œã€åˆ†é¡ç²¾åº¦ã¯ 90% ç¨‹åº¦ã¨ãªã£ãŸã€‚å®Ÿè¡Œæ™‚é–“ã¯æœ€è¿‘ã® PC ã§ç´„ 4 åˆ†ç¨‹åº¦ã¨çŸ­ãã€ã¡ã‚‡ã£ã¨æ‰‹ç›´ã—ã—ã¦å†å®Ÿé¨“ã™ã‚‹ã“ã¨ã‚‚æ°—æ¥½ã«ã§ãã‚‹ã€‚

# Appendix: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚·ãƒ•ãƒˆå‰‡ã¨ã‚³ã‚¹ãƒˆé–¢æ•°ã®å‹¾é…

æœ€å¾Œã«å‹¾é…ã®è¨ˆç®—ã«ã¤ã„ã¦è¦‹ã¦ãŠã“ã†ã€‚Maria Schuld et al. ã® [Evaluating analytic gradients on quantum hardware](https://journals.aps.org/pra/abstract/10.1103/PhysRevA.99.032331) ã‚„å¾Œç¶šã®è«–æ–‡ã«ã‚ˆã‚‹ã¨ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä»˜ãã® Pauli å›è»¢ã‚²ãƒ¼ãƒˆã‹ã‚‰ãªã‚‹ PQC (ã¾ãŸã¯ ansatz) $\ket{\psi(\theta)}$ ã§ã®ãƒãƒŸãƒ«ãƒˆãƒ‹ã‚¢ãƒ³ $H$ ã®æœŸå¾…å€¤ $f(\theta) := \braket{\psi(\theta) | H | \psi(\theta)}$ ã®å¾®åˆ†ã¯

$$
\begin{align*}
\frac{\partial}{\partial \theta_i} f(\theta) = \frac{f\left(\theta + \frac{1}{2}\right) - f\left(\theta - \frac{1}{2}\right)}{2}
\end{align*}
$$

ã§ä¸ãˆã‚‰ã‚Œã‚‹ã€‚

ã‚ˆã£ã¦ã€ä»Šå›ã®ã‚ˆã†ãª $N$ å€‹ã®è¦ç´ ã‹ã‚‰ãªã‚‹ãƒãƒƒãƒã§ã® MSE $\frac{1}{N} \sum|f(\theta) - y|^2$ ã®å‹¾é…ã¯

$$
\begin{align*}
\sum \frac{f(\theta) - y}{N} (f\left(\theta_1 + 1/2\right) - f\left(\theta_1 - 1/2\right), f\left(\theta_2 + 1/2\right) - f\left(\theta_2 - 1/2\right), \cdots)
\end{align*}
$$

ã§ä¸ãˆã‚‰ã‚Œã‚‹ã€‚

# ã¾ã¨ã‚

æ—¢ã« [Quantum Neural Networks](https://qiskit.org/ecosystem/machine-learning/tutorials/01_neural_networks.html) ã‹ã‚‰å§‹ã¾ã‚‹ä¸€é€£ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã€‚ä½†ã—ã€äº‹ä¾‹ãŒå°‘ãªãä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«é©ç”¨ã—ãŸã„å ´åˆãŒã‚„ã‚„ä¸é€æ˜ã§ã€ã¾ãŸã€æœ€é©åŒ–ã‚’ SDK ã«ä»»ã›ã‚‹ã®ã§ãã®ä¸‹ãŒã©ã†ãªã£ã¦ã„ã‚‹ã®ã‹åˆ†ã‹ã‚Šã«ãã„éƒ¨åˆ†ã‚‚ã‚ã‚‹ã¨æ„Ÿã˜ã‚‹ã€‚

ãã®ã‚ˆã†ãªèƒŒæ™¯ãŒã‚ã£ã¦ã€è‡ªåˆ†ã§ã‚‚ç†è§£ã‚’æ·±ã‚ã‚‹ãŸã‚ã«è¨“ç·´ãƒ«ãƒ¼ãƒ—ã‚‚å®Ÿè£…ã™ã‚‹å½¢ã§å®Ÿè£…ã‚’è¡Œã£ãŸã€‚ãªãŠã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚·ãƒ•ãƒˆå‰‡ã¯ä¸Šè¨˜ã®é€šã‚Šã«ã‚·ãƒ³ãƒ—ãƒ«ãªå®Ÿè£…ã§æ¸ˆã‚€ã®ã§è‡ªå‰å®Ÿè£…ã‚’è¡Œã„ã€`EstimatorQNN` ã®ä»£ã‚ã‚Šã« `Estimator` ã‚’ä½¿ã£ã¦ã‚‚åŒæ§˜ã®çµæœã‚’å¾—ã‚‰ã‚Œã‚‹ã€‚ä½†ã—ã€ãã‚Œã¯ `EstimatorQNN` ã‚’å†ç™ºæ˜ã™ã‚‹ã‚ˆã†ãªã‚‚ã®ãªã®ã§ã€å®Ÿè£…ãŒé•·ããªã‚Šè¦‹é€šã—ãŒæ‚ªããªã‚‹ã€‚ã‚ˆã£ã¦ã€ä»Šå›ã¯ç´ ç›´ã« `EstimatorQNN` ã‚’ç”¨ã„ãŸã€‚
