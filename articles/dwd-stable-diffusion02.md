---
title: "Stable Diffusion ã§éŠã‚“ã§ã¿ã‚‹ (2) â€” Diffusers ã®ä¸­ã‚’ã»ã‚“ã®è»½ãã ã‘è¦‹ã‚‹"
emoji: "ğŸ¨"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["Python", "æ©Ÿæ¢°å­¦ç¿’", "stablediffusion"]
published: true
---

# ç›®çš„

Hugging Face ã® [Diffusers](https://github.com/huggingface/diffusers) ã®ä¸­ã‚’æœ¬å½“ã«è»½ãã€é›°å›²æ°—ã ã‘è¦‹ã¦ã¿ãŸã„ã€‚

# Diffusers ã®è§£èª¬ã‚’è¦‹ã‚‹

GitHub ã¨å…¬å¼ãƒšãƒ¼ã‚¸ [Diffusers](https://huggingface.co/docs/diffusers/index) ã«ã‚ˆã‚‹ã¨ã€

> Diffusers: State-of-the-art diffusion models for image and audio generation in PyTorch and FLAX.

> Diffusers is the go-to library for state-of-the-art pretrained diffusion models for generating images, audio, and even 3D structures of molecules

ã¨ã„ã†ã“ã¨ã§ã€Stable Diffusion ã®ã‚ˆã†ãªç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã‚’ãŠæ‰‹è»½ã«æ‰±ãˆã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã‚ã‚‹ã€‚

> Our library is designed with a focus on usability over performance, simple over easy, and customizability over abstractions.

ä½¿ã„ã‚„ã™ã•ç­‰ãŒå„ªå…ˆãªã®ã§ã€ç‰©å‡„ã„ã‚·ãƒ“ã‚¢ãªè¦ä»¶ãŒã‚ã‚‹å ´åˆã¯è‡ªä½œã‚‚å¿…è¦ã‹ã‚‚ã—ã‚Œãªã„ã€‚

# Stable Diffusion ã«ã¤ã„ã¦è»½ãè¦‹ã‚‹

è«–æ–‡ [arXiv:2112.10752 High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752) (LDM)[^1]ã‚’ä½¿ç”¨ã—ãŸç”Ÿæˆãƒ¢ãƒ‡ãƒ«ã¨ã„ã†ã“ã¨ã«ãªã‚‹ã§ã‚ã‚ã†ã€‚[CompVis/stable-diffusion](https://github.com/CompVis/stable-diffusion) ã‚„ [Stability-AI/stablediffusion](https://github.com/Stability-AI/stablediffusion) ãŒ Stable Diffusion ã®å®Ÿè£…ã¨ã„ã†ã“ã¨ã«ãªã‚‹ã®ã ã‚ã†ã‹ã€‚

[^1]: å¯¾å¿œã‚³ãƒ¼ãƒ‰ [CompVis/latent-diffusion](https://github.com/CompVis/latent-diffusion)

å…¬å¼ã‚µã‚¤ãƒˆã®çµµçš„ã«ä»¥ä¸‹ã®ã‚ˆã†ãªã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ã®ã“ã¨ã§ã‚ã‚‹ã€‚

![](/images/dwd-stable-diffusion02/modelfigure.png)

ç·‘ã®éƒ¨åˆ†ã‚’ãƒ–ãƒ©ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹ã¨æ€ã†ã¨ã€

$$
\begin{align*}
z &= \mathcal{E}(x) \\
z &\mapsto z^\prime \\
\tilde{x} &= \mathcal{D}(z^\prime)
\end{align*}
$$

ã®ã‚ˆã†ãªå½¢ã«ãªã£ã¦ã„ã¦ã€â€œVAEâ€ ã®ã‚ˆã†ãªæ§‹é€ ã«ãªã£ã¦ã„ã‚‹ã€‚çœŸã‚“ä¸­ã® $z \mapsto z^\prime$ ã¯æ½œåœ¨ç©ºé–“ã®ä¸­ã§ã®æŒ™å‹•ã§ã‚ã‚Šã€[arXiv:2006.11239 Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) (DDPM) ã® Figure 2 ã®ã‚ˆã†ãªæ‹¡æ•£ãƒ¢ãƒ‡ãƒ«ã«ãªã£ã¦ã„ã‚‹ã€‚4 Experiments ã®ã¨ã“ã‚ã«

> To represent the reverse process, we use a U-Net backbone similar to an unmasked PixelCNN++ [52, 48] with group normalization throughout [66]. Parameters are shared across time, which is specified to the network using the Transformer sinusoidal position embedding [60]. We use self-attention at the 16 Ã— 16 feature map resolution [63, 60]. Details are in Appendix B.

ã¨ã‚ã‚‹ã‚ˆã†ã«ã€æ™‚é–“æƒ…å ±ã‚’ Transformer ã®ä½ç½®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§ã‚³ãƒ¼ãƒ‰åŒ–ã—ã¦ã€U-Net ã®ç•³ã¿è¾¼ã¿ãƒ–ãƒ­ãƒƒã‚¯ã«å·®ã—è¾¼ã‚€å®Ÿè£…ã‚’ã‚ˆãè¦‹ã‹ã‘ã‚‹ã‚ˆã†ã«æ€ã†ã€‚ã“ã®è¾ºã¯è§£èª¬ã‚’ 100 å›èª­ã‚€ã‚ˆã‚Šã¯ã€1 å›ã ã‘ã§ååˆ†ãªã®ã§ [ã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹Deep Learning âº â€•ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç·¨](https://www.oreilly.co.jp//books/9784814400591/) ã‚’å†™çµŒã—ã¤ã¤è§£èª¬ã‚’èª­ã‚€ã®ãŒä¸€ç•ªç†è§£ã§ãã‚‹æ°—ãŒã™ã‚‹[^2]ã€‚

[^2]: DDPM è«–æ–‡ã® Eq. (11) ã® $\mu_\theta (\mathbf{x}_t, t) = \frac{1}{\sqrt{\alpha_t}} \left( \mathbf{x}_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}} \, \epsilon_\theta (\mathbf{x}_t, t) \right)$ ã«ãŠã‘ã‚‹ $\epsilon_\theta (\mathbf{x}_t, t)$ ã®éƒ¨åˆ†ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ãŒ U-Net ã§å®Ÿè£…ã•ã‚Œã‚‹ã“ã¨ã«ãªã‚‹ãŒã€ã“ã®å¼ãŒã©ã®ã‚ˆã†ã«ã—ã¦å‡ºã¦æ¥ã¦ã€ä½•ã‚’ã—ã‚ˆã†ã¨ã—ã¦ã„ã‚‹ã®ã‹ã¯ãªã‹ãªã‹è«–æ–‡ã ã‘ã‹ã‚‰èª­ã¿å–ã‚‹ã®ã¯å›°é›£ãã†ã«æ„Ÿã˜ãŸã€‚

Latent Diffusion ã§ã¯ã€ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªæƒ…å ±ï¼ˆ`text-to-image` ã§ã‚ã‚Œã°ãƒ†ã‚­ã‚¹ãƒˆï¼‰ã§æ¡ä»¶ä»˜ã‘ã‚’ã™ã‚‹ãŸã‚ã«ã€æ¡ä»¶ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã¦ãã‚Œã‚‚ U-Net ã®ç•³ã¿è¾¼ã¿ãƒ–ãƒ­ãƒƒã‚¯ã«å·®ã—è¾¼ã‚€å½¢ã«ãªã£ã¦ã„ã‚‹ã‚ˆã†ã ã€‚ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ãªã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ã¨ã„ã†ã¨ [arXiv:2103.00020 Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020) (CLIP) ãŒæœ‰åã§ã‚ã‚ã†ã‹ã€‚

# Diffuser ã‚’è¦‹ã¦ã„ã

Diffusers ã® GitHub ã® README ã‚’ã•ã£ã¨è¦‹ã‚“ã§ä»¥ä¸‹ã®ã‚ˆã†ãªã‚³ãƒ¼ãƒ‰ã‚’æ›¸ã„ã¦ã¿ãŸã€‚æœ‰åãªé¦¬ã«ä¹—ã£ã¦ã„ã‚‹ã‚„ã¤ã§ã¯ãªãã€ã¡ã‚‡ã£ã¨é³¥ã®ã‚ˆã†ã«ç©ºã‚’é£›ã°ã›ã¦ã¿ãŸããªã£ãŸã€‚

```python
from diffusers import DiffusionPipeline
import torch


pipeline = DiffusionPipeline.from_pretrained(
    "runwayml/stable-diffusion-v1-5", torch_dtype=torch.float16
)
pipeline.to("cuda")

prompt = "a photo of an astronaut flying like a bird in the sky of the mars"
image = pipeline(prompt).images[0]
display(image)
```

![](/images/dwd-stable-diffusion02/001.png)

æ¥½ã—ãã†ï¼ˆï¼Ÿï¼‰ã§ã‚ã‚‹ã€‚

## pipeline

ä½œæˆã—ãŸ `pipeline` ã‚’å‡ºåŠ›ã—ã¦ã¿ã‚‹ã¨ä»¥ä¸‹ã®ã‚ˆã†ã«ãªã£ã¦ã„ãŸã€‚`transformers` ã¨ã‹ `CLIP` ã¨ã„ã†æ–‡å­—åˆ—ãŒè¦‹ãˆã‚‹ã€‚

```python
print(pipeline)
```

```
StableDiffusionPipeline {
  "_class_name": "StableDiffusionPipeline",
  "_diffusers_version": "0.29.0",
  "_name_or_path": "runwayml/stable-diffusion-v1-5",
  "feature_extractor": [
    "transformers",
    "CLIPImageProcessor"
  ],
  "image_encoder": [
    null,
    null
  ],
  "requires_safety_checker": true,
  "safety_checker": [
    "stable_diffusion",
    "StableDiffusionSafetyChecker"
  ],
  "scheduler": [
    "diffusers",
    "PNDMScheduler"
  ],
  "text_encoder": [
    "transformers",
    "CLIPTextModel"
  ],
  "tokenizer": [
    "transformers",
    "CLIPTokenizer"
  ],
  "unet": [
    "diffusers",
    "UNet2DConditionModel"
  ],
  "vae": [
    "diffusers",
    "AutoencoderKL"
  ]
}
```

## ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¨ãƒ†ã‚­ã‚¹ãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ€

ãƒ†ã‚­ã‚¹ãƒˆåŸ‹ã‚è¾¼ã¿ã‚’ã©ã†ã‚„ã£ã¦ä½œã£ã¦ã„ã‚‹ã®ã‹ã‚’å¿µã®ãŸã‚ç¢ºèªã—ãŸã„ã€‚

```python
print(type(pipeline.tokenizer))
print(type(pipeline.text_encoder))
```

> <class 'transformers.models.clip.tokenization_clip.CLIPTokenizer'>
> <class 'transformers.models.clip.modeling_clip.CLIPTextModel'>

ä¸Šã®ã»ã†ã§æ—¢ã«æ–‡å­—åˆ—ã§è¦‹ãˆã¦ã„ã‚‹ãŒ CLIP ã§å‡¦ç†ã—ã¦ã„ã‚‹ã‚ˆã†ã§ã‚ã‚‹[^3]ã€‚

[^3]: LDM ã®è«–æ–‡ä¸­ã§ã‚‚å°‘ã—åå‰ãŒå‡ºã¦ã„ã‚‹ã€‚

## ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹

ä¸€æ—¦ç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã¨å‹•çš„ã«ã‚»ãƒƒãƒˆã•ã‚Œã‚‹å±æ€§ã¨ã—ã¦ `do_classifier_free_guidance` ã¨ã„ã†ã®ãŒã‚ã£ã¦ã€ã“ã‚Œã‚’è¡¨ç¤ºã—ã¦ã¿ã‚‹:

```python
print(pipeline.do_classifier_free_guidance)
```

> True

ã¨ã„ã†ã“ã¨ã§ã€ã„ã‚ã‚†ã‚‹ã€Œåˆ†é¡å™¨ãªã—ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã€ã‚’ã‚„ã£ã¦ã„ã‚‹ã‚ˆã†ã§ã‚ã‚‹ã€‚OpenAI ã®ã€Œåˆ†é¡å™¨ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã€([arXiv:2105.05233 Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233)) ã¨ Google Research ã®ã€Œåˆ†é¡å™¨ãªã—ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã€([arXiv:2207.12598 Classifier-Free Diffusion Guidance](https://arxiv.org/abs/2207.12598)) ã¨ç›®ã¾ãã‚‹ã—ã„ã€‚

## U-Net

U-Net éƒ¨åˆ†ã‚’è¦‹ã¦ã¿ãŸã„

```python
print(pipeline.unet)
```

ã™ã‚‹ã¨

```
UNet2DConditionModel(
  (conv_in): Conv2d(4, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
  (time_proj): Timesteps()
  (time_embedding): TimestepEmbedding(
    (linear_1): Linear(in_features=320, out_features=1280, bias=True)
    (act): SiLU()
    (linear_2): Linear(in_features=1280, out_features=1280, bias=True)
  )
  ...
)
```

ã¨ã„ã†ã“ã¨ã§ [diffusers/models/unets/unet_2d_condition.py#L70-L1305](https://github.com/huggingface/diffusers/blob/v0.29.0/src/diffusers/models/unets/unet_2d_condition.py#L70-L1305) ã® `UNet2DConditionModel` ã‚¯ãƒ©ã‚¹ãŒä½¿ã‚ã‚Œã¦ã„ã‚‹ã€‚[arXiv:1505.04597](https://arxiv.org/abs/1505.04597) ã®é ƒã®ç´ æœ´ãª U-Net ã«æ¯”ã¹ã¦éšåˆ†ã¨å¤§å¤‰ãªä»£ç‰©ã«ãªã£ã¦ã„ã‚‹ã®ãŒçªºãˆã‚‹ã€‚

# ã¾ã¨ã‚

ã–ã£ã¨è¦‹ãŸã ã‘ã ãŒã€[ã‚¼ãƒ­ã‹ã‚‰ä½œã‚‹Deep Learning âº â€•ç”Ÿæˆãƒ¢ãƒ‡ãƒ«ç·¨](https://www.oreilly.co.jp//books/9784814400591/) ã§ã€Œåˆ†é¡å™¨ãªã—ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã€ãã‚‰ã„ã®æŠ€è¡“ã¾ã§ã®æ‹¡æ•£ãƒ¢ãƒ‡ãƒ«å‘¨è¾ºã«ã¤ã„ã¦ã–ã£ã¨å­¦ã‚“ã§ LDM ã®è«–æ–‡ [arXiv:2112.10752 High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752) ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨è¦‹æ¯”ã¹ã‚‹ã¨ï¼ˆè‰²ã€…ã‚„ã£ã¦ã„ã‚‹ã®ã§ã‚´ãƒãƒ£ã‚´ãƒãƒ£ã—ã¦ã„ã‚‹éƒ¨åˆ†ã¯ã‚ã‚‹ã«ã›ã‚ˆï¼‰è«–æ–‡ã«å‡ºã¦æ¥ã‚‹ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒä¸¦ã‚“ã§ã„ã‚‹ã®ã ãªã¨æ€ã†ã€‚

è»½ãã§ã‚‚çœºã‚ã¦æ–‡ç« åŒ–ã™ã‚‹ã¨ã€å°‘ã—è¦‹é€šã—ãŒè‰¯ããªã‚‹æ°—ãŒã™ã‚‹ã€‚
