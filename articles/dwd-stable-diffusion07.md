---
title: "Stable Diffusion ã§éŠã‚“ã§ã¿ã‚‹ (7) â€” FLUX.1 ã‚’è©¦ã—ã¦ã¿ã‚‹"
emoji: "ğŸ¨"
type: "tech" # tech: æŠ€è¡“è¨˜äº‹ / idea: ã‚¢ã‚¤ãƒ‡ã‚¢
topics: ["Python", "æ©Ÿæ¢°å­¦ç¿’", "stablediffusion"]
published: false
---

# ç›®çš„

ä¸–é–“ã§ [black-forest-labs/FLUX.1-dev](https://huggingface.co/black-forest-labs/FLUX.1-dev) ãŒæµè¡Œã£ã¦ã„ã‚‹ã‚‰ã—ã„ã®ã§ã€N ç•ªç…ã˜ã§å‹•ã‹ã—ã¦è‡ªå·±æº€è¶³ã—ãŸã¾ã¨ã‚ã€‚

# FLUX.1-dev-gguf

å™‚ã§ã¯å·¨å¤§ãªãƒ¢ãƒ‡ãƒ«ã‚‰ã—ã„ã®ã§ã€æã‚‹æã‚‹ã¨ã„ã†ã“ã¨ã§é‡å­åŒ–ã•ã‚ŒãŸãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‹ã‚‰ã„ããŸã„ã€‚`gguf` ãƒ•ã‚¡ã‚¤ãƒ«ã¯ `diffusers` ã§ã¯ç›´æ¥ãƒ­ãƒ¼ãƒ‰ã§ããšã€[leejet/stable-diffusion.cpp](https://github.com/leejet/stable-diffusion.cpp) ã‚’çµŒç”±ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã‚‰ã—ã„ã®ã§ã“ã‚Œã§è©¦ã™ã€‚

å¿µã®ãŸã‚ VRAM 24 GB ã® NVIDIA L4 ã‚’ä½¿ã£ãŸãŒã€12359MiB ãã‚‰ã„ã®æ¶ˆè²»é‡ã®ã‚ˆã†ã ã£ãŸã®ã§ã€ãŸã¶ã‚“ãã“ã¾ã§ VRAM ã‚’ä½¿ã£ã¦ã„ãªã„ã®ã§ã¯ãªã„ã‹ï¼Ÿã¨ã„ã†æ°—ãŒã™ã‚‹ã€‚

## OpenBLAS / cuBLAS ã®æº–å‚™

```sh
$ ldconfig -p | grep openblas
```

ã§ OpenBLAS ãŒå…¥ã£ã¦ã„ãªã•ãã†ãªã‚‰

```sh
$ sudo apt-get install libopenblas-dev
```

ã§ã‚‚ã—ã¦å°å…¥ã™ã‚‹ã€‚

```sh
$ cat /usr/local/cuda/include/cublas_api.h | grep CUBLAS_VER
```

ã§ `cuBLAS` ãŒå…¥ã£ã¦ã„ã‚‹ãªã‚‰ãã‚Œã§ã‚‚è‰¯ã„ã‚‰ã—ã„ã€‚

## stable-diffusion.cpp ã®ãƒ“ãƒ«ãƒ‰

ä»¥ä¸‹ã§ãƒ“ãƒ«ãƒ‰ã€‚OpenBLAS ã‚’ä½¿ã†ãªã‚‰ã€Œ-DSD_CUBLAS=ONã€ã‚’ã€Œ-DGGML_OPENBLAS=ONã€ã§è‰¯ã„ã‚‰ã—ã„[^1]ã€‚

[^1]: ãƒ“ãƒ«ãƒ‰ã¯ 1 æ™‚é–“ä»¥ä¸Šã‹ã‹ã£ãŸæ°—ãŒã™ã‚‹ãŒã‚ã¾ã‚Šã‚ˆãè¦šãˆã¦ã„ãªã„ã€‚

```sh
$ sudo apt install ccache
$ git clone --recursive https://github.com/leejet/stable-diffusion.cpp
$ cd stable-diffusion.cpp
$ mkdir build
$ cd build
$ cmake .. -DSD_CUBLAS=ON
$ cmake --build . --config Release
```

> [  1%] Building C object thirdparty/CMakeFiles/zip.dir/zip.c.o
> In file included from /home/xxx/git_work/stable-diffusion.cpp/thirdparty/zip.c:40:
> /home/xxx/git_work/stable-diffusion.cpp/thirdparty/miniz.h:4988:9: note: â€˜#pragma message: Using fopen, ftello, fseeko, stat() etc. path for file I/O - this path may not support large files.â€™
>  4988 | #pragma message(                                                               \
>       |         ^~~~~~~
> [  1%] Built target zip
> ...
> [ 98%] Building CXX object examples/cli/CMakeFiles/sd.dir/main.cpp.o
> [100%] Linking CXX executable ../../bin/sd
> [100%] Built target sd

## ãƒ¢ãƒ‡ãƒ«ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

è‰²ã€…å‚è€ƒã«ã—ãŸã®ã§è©³ç´°ã¯å¿˜ã‚ŒãŸãŒä»¥ä¸‹ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€‚

```sh
$ mkdir models
$ cd models
$ curl -LO https://huggingface.co/city96/FLUX.1-dev-gguf/resolve/main/flux1-dev-Q8_0.gguf
$ curl -LO https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/ae.safetensors
$ curl -LO https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/clip_l.safetensors
$ curl -LO https://huggingface.co/comfyanonymous/flux_text_encoders/resolve/main/t5xxl_fp16.safetensors
```

```sh
$ du -h
...
22G     .
```

çµæ§‹ãªã‚µã‚¤ã‚ºã€‚

## ç”»åƒç”Ÿæˆ

ä»¥ä¸‹ã®ã‚ˆã†ãªæ„Ÿã˜ã§ç”»åƒç”Ÿæˆã—ãŸã€‚

```sh
./build/bin/sd --diffusion-model  models/flux1-dev-Q8_0.gguf \
  --vae ./models/ae.safetensors --clip_l ./models/clip_l.safetensors \
  --t5xxl ./models/t5xxl_fp16.safetensors \
  -p "A muscular macho male warrior wearing golden armor is holding a large sword. The sword is covered with flames. He is now fighting with a dragon." \
  --cfg-scale 1.0 --sampling-method euler -v
```

![](/images/dwd-stable-diffusion07/001.png =400x)

ç‚ã‚’ã¾ã¨ã£ãŸå‰£ã‚’ç´ æ‰‹ã§ç›´æ¥æŒã£ã¡ã‚ƒãƒ€ãƒ¡ã§ã™ã‚ˆï½ã£ã¦ã®ã¯ã‚ã‚‹ã‘ã©ã€ç¶ºéº—ãªç”»åƒãŒå‡ºæ¥ãŸã€‚

# FLUX.1-dev

ã‚„ã¯ã‚Š `diffusers` çµŒç”±ã§ä½¿ã„ãŸã„ã®ã§ã‚‚ã†å°‘ã—èª¿æŸ»ã€‚[sayakpaul/flux.1-dev-nf4-with-bnb-integration](https://huggingface.co/sayakpaul/flux.1-dev-nf4-with-bnb-integration) ã‚„ [[Quantization] Add quantization support for `bitsandbytes` #9213](https://github.com/huggingface/diffusers/pull/9213) ã®å†…å®¹ãŒè‰¯ã•ãã†ã ã£ãŸã®ã§ã€ã“ã‚Œã‚’è©¦ã—ãŸã€‚æ­£ç¢ºã«ã¯å‰è€…ã®ãƒšãƒ¼ã‚¸ã«ä»¥ä¸‹ã®ãƒªãƒ³ã‚¯ãŒã‚ã£ã¦ã€ã“ã®å…ˆã® `sayakpaul/flux.1-dev-nf4-pkg` ã‚’è©¦ã—ãŸã€‚

> Check out [sayakpaul/flux.1-dev-nf4-pkg](https://huggingface.co/sayakpaul/flux.1-dev-nf4-pkg) that shows how to run this checkpoint along with an NF4 T5 in a free-tier Colab Notebook.

FLUX.1-dev ã‚’ 16 GB ã® VRAM ã§å‹•ã‹ã™ã¨ã„ã†å†…å®¹ã ãŒã€Colab ä»¥å¤–ã® 16 GB ç’°å¢ƒã§ã¯ã‚ã‚‹ã‚ªãƒã‚¸ãƒŠã‚¤ãŒå¿…è¦ã ã£ãŸã€‚15098MiB ãã‚‰ã„ VRAM ã‚’ä½¿ã£ã¦ã„ã¦ã€ç”»åƒã‚’å‡ºåŠ›ã™ã‚‹ç›´å‰ã§ 

> OutOfMemoryError: CUDA out of memory.

ãŒç™ºç”Ÿã—ãŸã®ã ã€‚ãªãŠã€å¿…è¦ãªã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚µã‚¤ã‚ºã¯ 16 GB ãã‚‰ã„ã ã£ãŸã€‚ã¤ã¾ã‚Šã€16 GB ãã‚‰ã„ã®ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã¨ 16 GB ãã‚‰ã„ã® VRAM ãŒå¿…è¦ã«ãªã‚‹ã€‚

## æº–å‚™

ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ã€é–‹ç™ºä¸­ã® `diffusers` ã‚’å°å…¥ã™ã‚‹ã€‚

```sh
$ pip install bitsandbytes
$ pip install sentencepiece
$ pip install -U git+https://github.com/huggingface/diffusers@c795c82df39620e2576ccda765b6e67e849c36e7
```

## JupyterLab ã‚’èµ·å‹•

ä»Šå› JupyterLab ã‚’ä½¿ã£ãŸãŒã€T4 / A4000 ã®ã‚ˆã†ã« VRAM ãŒ 16 GB ã®å ´åˆã«ã¯ã‚ªãƒã‚¸ãƒŠã‚¤ã‚’ã‹ã‘ã¦ã‹ã‚‰èµ·å‹•ã—ãŸã€‚[Optimizing memory usage with `PYTORCH_CUDA_ALLOC_CONF`](https://pytorch.org/docs/stable/notes/cuda.html#optimizing-memory-usage-with-pytorch-cuda-alloc-conf) ã«ã‚ˆã‚‹ã¨ã€VRAM ã®ä½¿ã„æ–¹ã‚’æœ€é©åŒ–ã™ã‚‹æ„Ÿã˜ã®ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚‰ã—ã„ã€‚L4 ã®ã‚ˆã†ã« VRAM ãŒ 24 GB ãã‚‰ã„ä½¿ãˆã‚‹ãªã‚‰ã‚ªãƒã‚¸ãƒŠã‚¤ãªã—ã§ã‚‚ã„ã‘ã‚‹ã€‚

```sh
$ export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
$ jupyter lab
```

## Colab Notebook ã®é€šã‚Šã«å®Ÿè¡Œ

[sayakpaul/flux.1-dev-nf4-pkg](https://huggingface.co/sayakpaul/flux.1-dev-nf4-pkg) ã‹ã‚‰ãƒªãƒ³ã‚¯ã•ã‚Œã¦ã„ã‚‹ Colab Notebook ã‚’ãã®ã¾ã¾ãƒ­ãƒ¼ã‚«ãƒ«å®Ÿè¡Œã™ã‚‹ã ã‘ã§ã‚ã‚‹ã€‚

```python
from transformers import T5EncoderModel
from diffusers import FluxTransformer2DModel, FluxPipeline
import torch
import gc


def flush():
    """Wipes off memory."""
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.reset_max_memory_allocated()
    torch.cuda.reset_peak_memory_stats()


def bytes_to_giga_bytes(bytes):
    return f"{(bytes / 1024 / 1024 / 1024):.3f}"

flush()
```

ãã®ã¾ã¾ç¶šã„ã¦ã€

```python
%%time

nf4_model_id = "sayakpaul/flux.1-dev-nf4-pkg"
text_encoder_2 = T5EncoderModel.from_pretrained(
    nf4_model_id, subfolder="text_encoder_2", torch_dtype=torch.float16
)
transformer = FluxTransformer2DModel.from_pretrained(
    nf4_model_id, subfolder="transformer", torch_dtype=torch.float16
)
```

ã“ã“ã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã§ 4ï½8 åˆ†ãã‚‰ã„ã‹ã‹ã‚‹ã¨æ€ã‚ã‚Œã‚‹ã€‚ãƒˆãƒ¼ã‚¿ãƒ« 13 GB ãã‚‰ã„ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãŒç™ºç”Ÿã™ã‚‹ã€‚

```python
%%time

pipe = FluxPipeline.from_pretrained(
    "black-forest-labs/FLUX.1-dev",
    text_encoder_2=text_encoder_2,
    transformer=transformer,
    torch_dtype=torch.float16
)
pipe.enable_model_cpu_offload()
```

ã“ã“ã¯ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã« 2 åˆ†ãã‚‰ã„ã‹ã‹ã‚‹ã€‚

## ç”»åƒç”Ÿæˆ

```python
%%time

prompt = "An anime-style muscular macho male warrior wearing golden armor is holding a large sword. The sword is covered with flames. He is now fighting against a dragon."

image = pipe(
    prompt,
    guidance_scale=3.5,
    num_inference_steps=50,
    generator=torch.manual_seed(0)
).images[0]

torch.cuda.empty_cache()
memory = bytes_to_giga_bytes(torch.cuda.memory_allocated())
print(f"{memory=} GB.")

image.resize((image.size[0] // 3, image.size[1] // 3))
```

![](/images/dwd-stable-diffusion07/002.png =400x)

è‰¯ã„æ„Ÿã˜ã€‚A4000 ã§ 1min 40s ãã‚‰ã„ã§ã€L4 ã§ 1min 57s ãã‚‰ã„ã§ç”»åƒãŒç”Ÿæˆã•ã‚ŒãŸã€‚

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ `prompt = "An anime-style winged goddess flies in the sky and watches over the earth."` ã«ã—ã¦ã€`seed` ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã—ã¦ä½•åº¦ã‹ã‚¬ãƒãƒ£ã‚’å¼•ã„ãŸã‚‰ä»¥ä¸‹ã®ã‚ˆã†ãªç”»åƒãŒç”Ÿæˆã•ã‚ŒãŸã€‚ãªã‹ãªã‹ãã‚Œã£ã½ã„ã€‚

![](/images/dwd-stable-diffusion07/003.png =400x)

# ã¾ã¨ã‚

ä¸–é–“ã§æµè¡Œã£ã¦ã„ã‚‹ã‚‰ã—ã„ FLUX.1-dev ã‚’å®Ÿè¡Œã§ããŸã€‚ç›¸å¤‰ã‚ã‚‰ãšæŒ‡ãŒ 6 æœ¬ã«ãªã£ãŸã‚Šã™ã‚‹ã“ã¨ã¯ã‚ã‚‹ã—ã€æ™‚ã€…è¨€ã†ã“ã¨ã‚’èã„ã¦ãã‚Œãªã„ãƒ»ãƒ»ãƒ»ã¨ã„ã†ã‹ã€Œã†ï½ã‚“ã€ã£ã¦ã‚‚ã®ãŒç”Ÿæˆã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã¯ã”æ„›æ•¬ã€‚
